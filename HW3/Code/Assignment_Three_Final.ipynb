{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib import gridspec\n",
    "\n",
    "# Numerical Library\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import collections, math, operator\n",
    "\n",
    "from scipy.special import expit\n",
    "\n",
    "# Limit printout to 3 decimal places\n",
    "np.set_printoptions(precision=3,suppress=True)\n",
    "\n",
    "# Allows for printing inline for jupyter notebook\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Part 1\n",
    "DistanceFlagPair = collections.namedtuple('Distance', 'distance flag')\n",
    "\n",
    "class DataFormat():\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    def func_data_set(self):\n",
    "        src_data = genfromtxt(self.name, delimiter=',')\n",
    "        flag_data = src_data[:,0]\n",
    "        feature_data = np.delete(src_data, 0, axis=1)\n",
    "        return feature_data, flag_data\n",
    "        \n",
    "    def func_norm_data(self):\n",
    "        src_data = genfromtxt(self.name, delimiter=',')\n",
    "        flag_data, feature_data = src_data[:,0], np.delete(src_data, 0, axis=1)\n",
    "        feature_max, feature_min = np.amax(feature_data, axis=0), np.amin(feature_data, axis=0)\n",
    "        feature_range, feature_avg = feature_max - feature_min, np.average(feature_data)\n",
    "        norm_data = (feature_data - feature_avg) / feature_range\n",
    "        self.norm_data, self.flag_data = norm_data, flag_data\n",
    "        return norm_data, flag_data\n",
    "\n",
    "    def func_leave_out(self, index):\n",
    "        return self.norm_data[index], self.flag_data[index]\n",
    "\n",
    "class KNN():\n",
    "\n",
    "    def __init__(self, norm_data, flag_data):\n",
    "        self.norm_data, self.flag_data = norm_data, flag_data\n",
    "\n",
    "    def func_classify(self, test_data, k):\n",
    "        neigbor_pair = self.func_build_neigbor(test_data)\n",
    "        flag_sum = np.sum([neigbor.flag for neigbor in neigbor_pair[:k]])\n",
    "        return -1 if flag_sum < 0 else 1\n",
    "        \n",
    "    def func_build_neigbor(self, test_data):\n",
    "        diff_sqrt = (test_data - self.norm_data) ** 2\n",
    "        distance_data = np.sqrt(np.sum(diff_sqrt, axis=1))\n",
    "        neigbor_pair = []\n",
    "        for i in range(len(distance_data)):\n",
    "            neigbor = DistanceFlagPair(distance_data[i], self.flag_data[i])\n",
    "            neigbor_pair.append(neigbor)\n",
    "        return sorted(neigbor_pair, key=operator.attrgetter('distance'))        \n",
    "\n",
    "def func_calc_error(data, knn, k = 1):\n",
    "    temp_data = [knn.func_classify(classifier, k) for classifier in data.norm_data]\n",
    "    return np.sum(np.abs(temp_data - data.flag_data)) / float(2 * len(data.flag_data))\n",
    "    \n",
    "def func_cross_valid_error(data, knn, k = 1):\n",
    "    error = 0\n",
    "    for i in range(len(data.norm_data)):\n",
    "        norm, flag = data.func_leave_out(i)\n",
    "        flag_sum = knn.func_classify(norm, k)\n",
    "        flag_error = np.abs(flag - flag_sum) / 2\n",
    "        error += flag_error\n",
    "    return float(error) / (len(data.flag_data) + 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Part 2\n",
    "class DecisionStump():\n",
    "\n",
    "    def __init__(self, norm_data, flag_data):\n",
    "        self.norm_data, self.flag_data = norm_data, flag_data\n",
    "\n",
    "    def build_stump(self):\n",
    "        row, col = self.norm_data.shape\n",
    "        cur_info_gain, feature_num, split_num = 0, 0, 0\n",
    "        for index in range(col):\n",
    "            for value in self.norm_data.T[index]:\n",
    "                tmp_info_gain = self.info_gain(index, value)\n",
    "                if cur_info_gain < tmp_info_gain:\n",
    "                    cur_info_gain = tmp_info_gain\n",
    "                    feature_num, split_num = index, value\n",
    "        return cur_info_gain, feature_num, split_num\n",
    "\n",
    "    def info_gain(self, index, value):\n",
    "        pos, neg = func_count_one(self.flag_data)\n",
    "        init_entropy = func_calc_entropy(pos, neg)\n",
    "        row_count, col_count = self.norm_data.shape\n",
    "        upper_pos_count, upper_neg_count, lower_pos_count, lower_neg_count = self.count_one(index, value)\n",
    "        upper_entropy = func_calc_entropy(upper_pos_count, upper_neg_count)\n",
    "        upper_entropy = float(upper_pos_count + upper_neg_count) / row_count * upper_entropy\n",
    "        lower_entropy = func_calc_entropy(lower_pos_count, lower_neg_count)\n",
    "        lower_entropy = float(lower_pos_count + upper_neg_count) / row_count * lower_entropy\n",
    "        return init_entropy - upper_entropy - lower_entropy\n",
    "\n",
    "    def count_one(self, index, value):\n",
    "        upper_pos_count, upper_neg_count = 0, 0\n",
    "        lower_pos_count, lower_neg_count = 0, 0\n",
    "        norm_data_t = self.norm_data.T[index]\n",
    "        for i in range(len(norm_data_t)):\n",
    "            tmp_value = norm_data_t[i]\n",
    "            if tmp_value > value:\n",
    "                if self.flag_data[i] > 0: upper_pos_count += 1\n",
    "                else: upper_neg_count += 1\n",
    "            else:\n",
    "                if self.flag_data[i] > 0: lower_pos_count += 1\n",
    "                else: lower_neg_count += 1\n",
    "        return upper_pos_count, upper_neg_count, lower_pos_count, lower_neg_count\n",
    "\n",
    "    def calc_error_rate(self, feature_num, split_num):\n",
    "        predict, true, false = 0, 0, 0\n",
    "        for i in range(len(self.norm_data)):\n",
    "            predict = -1 if self.norm_data[i][feature_num] < split_num else 1\n",
    "            if predict == self.flag_data[i]: true += 1\n",
    "            else: false += 1\n",
    "        return float(true) / (true + false)\n",
    "\n",
    "class TreeNode():\n",
    "    \n",
    "    def __init__(self, column = -1, value = None, rs = None, left = None, right = None):\n",
    "        self.column, self.value = column, value\n",
    "        self.rs, self.left, self.right = rs, left, right\n",
    "    \n",
    "class DecisionTree():\n",
    "\n",
    "    def __init__(self, norm_data, flag_data):\n",
    "        self.norm_data, self.flag_data = norm_data, flag_data\n",
    "        \n",
    "    def info_gain(self):\n",
    "        pos, neg = func_count_one(self.flag_data)\n",
    "        init_entropy = func_calc_entropy(pos, neg)\n",
    "        row_count, col_count = self.norm_data.shape\n",
    "        upper_pos_count, upper_neg_count, lower_pos_count, lower_neg_count = self.count_one(index, value)\n",
    "        upper_entropy = func_calc_entropy(upper_pos_count, upper_neg_count)\n",
    "        upper_entropy = float(upper_pos_count + upper_neg_count) / row_count * upper_entropy\n",
    "        lower_entropy = func_calc_entropy(lower_pos_count, lower_neg_count)\n",
    "        lower_entropy = float(lower_pos_count + upper_neg_count) / row_count * lower_entropy\n",
    "        return init_entropy - upper_entropy - lower_entropy\n",
    "        \n",
    "    def build_tree(rows, score):\n",
    "        if not rows: return TreeNode()\n",
    "        cur_score, best_info_gain = 0, 0\n",
    "        best_option, best_set = None, None\n",
    "        for row in self.norm_data:\n",
    "            for index in range(len(row)):\n",
    "                \n",
    "\n",
    "def func_count_one(flag_data):\n",
    "    pos, neg = 0, 0\n",
    "    for data in flag_data:\n",
    "        if data == 1: pos += 1\n",
    "        if data == -1: neg += 1\n",
    "    return pos, neg\n",
    "\n",
    "def func_calc_entropy(pos, neg):\n",
    "    try:\n",
    "        lhs_prob, rhs_prob = float(pos) / (pos + neg), float(neg) / (pos + neg)\n",
    "        entropy = lhs_prob * np.log(lhs_prob) + rhs_prob * np.log(rhs_prob)\n",
    "        return -entropy\n",
    "    except ZeroDivisionError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Development\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:66: RuntimeWarning: divide by zero encountered in log\n",
      "D:\\Development\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:66: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Information Gain Value: 0.613224971014\n",
      "Training Feature Value: 22\n",
      "Training Error Rate: 0.933098591549\n",
      "\n",
      "Testing Information Gain Value: 0.606475151087\n",
      "Testing Feature Value: 27\n",
      "Testing Error Rate: 0.890845070423\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def func_plot_data(fig, x_list, y_list, label, title='Training-Testing Data'):\n",
    "    plt.plot(x_list, y_list, label=label)\n",
    "    plt.legend()\n",
    "    fig.suptitle(title + ' Error', fontsize=16)\n",
    "    plt.xlabel('K-Value', fontsize=16)\n",
    "    plt.ylabel('Error', fontsize=16)\n",
    "\n",
    "def func_part_one():\n",
    "    train_data, test_data = DataFormat('knn_train.csv'), DataFormat('knn_test.csv')\n",
    "    train_norm_data, train_flag_data = train_data.func_norm_data()\n",
    "    test_norm_data, test_flag_data = test_data.func_norm_data()\n",
    "    knn_train, knn_test = KNN(train_norm_data, train_flag_data), KNN(test_norm_data, test_flag_data)\n",
    "    k_list, train_error_list, cross_valid_list, test_error_list = range(1, 52, 2), [], [], []\n",
    "    \n",
    "    for k in k_list:\n",
    "        train_error = func_calc_error(train_data, knn_train, k)\n",
    "        train_error_list.append(train_error)\n",
    "        cross_valid = func_cross_valid_error(train_data, knn_train, k)\n",
    "        cross_valid_list.append(cross_valid)\n",
    "        test_error = func_calc_error(test_data, knn_test, k)\n",
    "        test_error_list.append(test_error)\n",
    "\n",
    "    fig = plt.figure(figsize=(9,6))\n",
    "    print 'Training:', train_error_list\n",
    "    func_plot_data(fig, k_list, train_error_list, 'Training Data')\n",
    "    print 'Testing:', test_error_list    \n",
    "    func_plot_data(fig, k_list, test_error_list, 'Testing Data')\n",
    "    fig = plt.figure(figsize=(9,6))\n",
    "    print 'Cross Valid:', cross_valid_list\n",
    "    func_plot_data(fig, k_list, cross_valid_list, 'Cross Validation', 'Cross Validation')\n",
    "\n",
    "def func_part_two():\n",
    "    train_data, test_data = DataFormat('knn_train.csv'), DataFormat('knn_test.csv')\n",
    "    train_norm_data, train_flag_data = train_data.func_norm_data()\n",
    "    test_norm_data, test_flag_data = test_data.func_norm_data()\n",
    "    ''' Decision Stump '''\n",
    "    train_stump, test_stump = DecisionStump(train_norm_data, train_flag_data), DecisionStump(test_norm_data, test_flag_data)\n",
    "    train_info_gain, train_feature, train_split = train_stump.build_stump()\n",
    "    test_info_gain, test_feature, test_split = test_stump.build_stump()\n",
    "    # Training Data Outputs\n",
    "    print 'Training Information Gain Value:', train_info_gain\n",
    "    print 'Training Feature Value:', train_feature\n",
    "    # print 'Training Splitted Value:', train_split\n",
    "    print 'Training Error Rate:', train_stump.calc_error_rate(train_feature, train_split)\n",
    "    print\n",
    "    # Testing Data outputs\n",
    "    print 'Testing Information Gain Value:', test_info_gain\n",
    "    print 'Testing Feature Value:', test_feature\n",
    "    # print 'Testing Splitted Value:', test_split\n",
    "    print 'Testing Error Rate:', test_stump.calc_error_rate(test_feature, test_split)\n",
    "    ''' Decision Tree '''\n",
    "    train_data = DecisionTree()    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "#     func_part_one()\n",
    "    func_part_two()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
